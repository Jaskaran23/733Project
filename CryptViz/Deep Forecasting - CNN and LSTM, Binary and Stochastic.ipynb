{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting the Cryptocurrency Ecosystem with Deep Learning\n",
    "In this notebook we will collect price history data for the top 100 coins by marketcap. We will chunk data into 2D tensors of 90x5 where 90 is number of days of input, and 5 is  the number of feature points per coin per day. Feature points are: ['Open', 'High', 'Low', 'Close', 'DayOfYear'].\n",
    "\n",
    "From this time series data, our model will learn to predict how prices will change 1, 10, 30, and 90 days in the future. Our experiments include both an LSTM and a CNN implementation, both of which take the same data as input. \n",
    "\n",
    "We will do two different types of price forecasting:\n",
    "1. Binary Prediction: \n",
    "For each coin, predict if the price of that coin will be higher or lower in n days.\n",
    "2. Linear Prediction:\n",
    "Predict the actual price change of a coin at a future date.\n",
    "3. Stochastic Forecasting: \n",
    "Produce a stochastic evaluation over all coins. That is, produce a vector of length equal to number of coins, whos elements are non-zero and sum to one. Each element of the vector is a scalar evaluation of a coin. This vector can be thought of as a confidence distribution over the coins, where the higher the evaluation, the higher the confidence of gain in price. This vector can also be used to simulate an investment portfolio, as a portfolio can be seen as a stochastiv vector of assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "DAYS_BLK = 90\n",
    "TARGETS  = [1, 10, 30, 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_samples(coins, DAYS_BLK, TARGETS):\n",
    "    # Compile a list of training examples, accompanied by target values for future prices\n",
    "    samples = []\n",
    "    for coin in tqdm.tqdm(coins):\n",
    "        # Get all available price history for a given coin\n",
    "        coin_data = coin.read_history()\n",
    "\n",
    "        # Drop unneccessary columns\n",
    "        coin_data = coin_data.drop(['Volume', 'Market Cap', 'Unnamed: 0'], 1) \n",
    "\n",
    "        # Reverse date ordering so we predict the future instead of the past\n",
    "        coin_data['Date'] = pd.to_datetime(coin_data['Date'])\n",
    "        coin_data = coin_data.sort_values(['Date']) \n",
    "\n",
    "        # Convert Date to Day of Year(DOY)\n",
    "        coin_data['DOY'] = coin_data['Date'].apply(lambda x: x.dayofyear)\n",
    "        coin_data = coin_data.drop(['Date'], 1)\n",
    "\n",
    "        # Chunk coin history into input/target pairs\n",
    "        for t in range(len(coin_data) - DAYS_BLK - TARGETS[-1]):\n",
    "\n",
    "            # X is a 90 day chunk of shape: (90, 5)\n",
    "            X = coin_data.iloc[t:t+DAYS_BLK].copy()\n",
    "            # y is a list of future prices [1, 10, 30, 90] days after X\n",
    "            y = [coin_data.iloc[t+DAYS_BLK+n]['Close'] for n in TARGETS]\n",
    "\n",
    "            # Normalize prices relative to the final input close price\n",
    "            price_cols = ['Open', 'High', 'Low', 'Close']\n",
    "            current_price = X.values[-1][3]\n",
    "            for col in price_cols:\n",
    "                X[col] = X[col] / current_price\n",
    "\n",
    "            # Normalize future prices by current price\n",
    "            y = [future_price / current_price for future_price in y]\n",
    "\n",
    "            # Normalize DOY column\n",
    "            X['DOY'] = X['DOY'] / 365\n",
    "\n",
    "            # Sample is ready\n",
    "            samples.append((X.values, y))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of coin objects for the top 100 coins from coinmarketcap.com\n",
    "from Scrapers.Coinmarketcap.coinmarketcap import CoinMarketcap\n",
    "cmk = CoinMarketcap()\n",
    "coins = cmk.coins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:07<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cache samples because compiling them is timely\n",
    "RECOMPILE_SAMPLES = True\n",
    "if RECOMPILE_SAMPLES:\n",
    "    samples = compile_samples(coins, DAYS_BLK, TARGETS)\n",
    "    pickle.dump(samples, open( \"dataset.pickle\", \"wb\" ) )\n",
    "else:\n",
    "    try:\n",
    "        print(\"Checking Cache...\")\n",
    "        samples = pickle.load(open('dataset.pickle','rb'))\n",
    "        print(\"Cached samples loaded!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Cached samples not found. Fetching data and compiling samples...\")\n",
    "        samples = compile_samples(coins, DAYS_BLK, TARGETS)\n",
    "        print(\"Caching samples...\")\n",
    "        pickle.dump(samples, open( \"dataset.pickle\", \"wb\" ) )\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36964"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten\n",
    "\n",
    "# Assert input shape\n",
    "N = 90 # Number of Days\n",
    "F = 5  # Number of Features: ['Open', 'High', 'Low', 'Close', 'DayOfYear']\n",
    "assert((N, F) == samples[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN2D(forecast='binary'):\n",
    "    # Define our Temporal Convolutional Model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=4, kernel_size=3, activation='relu', input_shape=[N,F]))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid')) # 'linear' for price prediction, 'softmax' for stochastic forecasting\n",
    "    \n",
    "    # Compile our Model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binary Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try predicting one day into the future\n",
    "\n",
    "# Split samples into inputs and targets\n",
    "X,y = zip(*samples)\n",
    "\n",
    "# target[0] is the price 1 day after input\n",
    "y = [target[0] for target in y]\n",
    "\n",
    "# convert target to boolean. 0 means price went down, 1 means price went up\n",
    "y = [int(target>1) for target in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29571 samples, validate on 7393 samples\n",
      "Epoch 1/10\n",
      "29571/29571 [==============================] - 2s 77us/step - loss: 0.6971 - mean_absolute_error: 0.4978 - val_loss: 0.6935 - val_mean_absolute_error: 0.4966\n",
      "Epoch 2/10\n",
      "29571/29571 [==============================] - 2s 63us/step - loss: 0.6920 - mean_absolute_error: 0.4968 - val_loss: 0.6920 - val_mean_absolute_error: 0.4982\n",
      "Epoch 3/10\n",
      "29571/29571 [==============================] - 2s 63us/step - loss: 0.6908 - mean_absolute_error: 0.4969 - val_loss: 0.6918 - val_mean_absolute_error: 0.4967\n",
      "Epoch 4/10\n",
      "29571/29571 [==============================] - 2s 64us/step - loss: 0.6899 - mean_absolute_error: 0.4963 - val_loss: 0.6919 - val_mean_absolute_error: 0.4959\n",
      "Epoch 5/10\n",
      "29571/29571 [==============================] - 2s 63us/step - loss: 0.6889 - mean_absolute_error: 0.4956 - val_loss: 0.6905 - val_mean_absolute_error: 0.4966\n",
      "Epoch 6/10\n",
      "29571/29571 [==============================] - 2s 64us/step - loss: 0.6882 - mean_absolute_error: 0.4952 - val_loss: 0.6922 - val_mean_absolute_error: 0.4959\n",
      "Epoch 7/10\n",
      "29571/29571 [==============================] - 2s 67us/step - loss: 0.6875 - mean_absolute_error: 0.4945 - val_loss: 0.6906 - val_mean_absolute_error: 0.4954\n",
      "Epoch 8/10\n",
      "29571/29571 [==============================] - 2s 65us/step - loss: 0.6867 - mean_absolute_error: 0.4941 - val_loss: 0.6901 - val_mean_absolute_error: 0.4942\n",
      "Epoch 9/10\n",
      "29571/29571 [==============================] - 2s 65us/step - loss: 0.6863 - mean_absolute_error: 0.4937 - val_loss: 0.6903 - val_mean_absolute_error: 0.4957\n",
      "Epoch 10/10\n",
      "29571/29571 [==============================] - 2s 65us/step - loss: 0.6856 - mean_absolute_error: 0.4932 - val_loss: 0.6902 - val_mean_absolute_error: 0.4938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f654897e748>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN2D()\n",
    "model.fit(np.array(X), np.array(y), epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Set Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convulutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stochastic Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Set Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "733Project",
   "language": "python",
   "name": "733project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
