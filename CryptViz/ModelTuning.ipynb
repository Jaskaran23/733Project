{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Forecasting: Predicting Cryptocurrency Prices with CNN, LSTM, and Transfer Learning\n",
    "\n",
    "### Introduction\n",
    "In this notebook we will collect price history data for the top 100 coins by marketcap. We will chunk data into 2D tensors of 60x5 where 60 is number of days of input, and 5 is  the number of feature points per coin per day. Feature points are: ['Open', 'High', 'Low', 'Close', 'DayOfYear']. From this time series data, our model will learn to predict how prices will change for 1, and 30, days in the future. Our experiments include both an LSTM and a CNN implementation, both of which take the same data as input. \n",
    "\n",
    "Original models will be trained for Binary Prediction, that is, to predict if a price will go up or down after n days. We will use transfer learning by retraining the final layers of the model for alternative forecast types. We will implement three forecast types Binary, Linear, and Stochastic; the only difference bieng their respective output activation functions: Sigmoid, Linear, and Softmax.\n",
    "\n",
    "### 1. Data Collection and Aquisition\n",
    "* Coinmarketcap module\n",
    "* Coin anonymity\n",
    "* Input Shape: (60,5)\n",
    "\n",
    "### 2. Model Definitions\n",
    "* CNN\n",
    "* LSTM\n",
    "\n",
    "### 3. Binary Forecasting\n",
    "* Binary Forecasting\n",
    "    * For each coin, predict if the price of that coin will be higher or lower in n days.\n",
    "    * Activation: sigmoid\n",
    "    * Target: sigmoid(future price / current price)\n",
    "\n",
    "### 4. Linear Forecasting with Transfer Learning\n",
    "* Linear Forecasting or Price Prediction:\n",
    "    * Predict the actual price of a coin at a future date.\n",
    "    * Additional Layer: Dense(21)\n",
    "    * Activation: linear\n",
    "    * Target: future price / current price\n",
    "\n",
    "### 5. Applications\n",
    "* Cryptocurrency Investment Recommendations\n",
    "    * A ranking of coin evaluations and predicted prices\n",
    "    \n",
    "### 6. Future Work\n",
    "* Stochastic Forecasting: \n",
    "    * Produce a stochastic evaluation over all coins. That is, produce a vector of length equal to number of coins, whos elements are non-zero and sum to one. Each element of the vector is a scalar evaluation of a coin. This vector can be thought of as a confidence distribution over the coins, where the higher the evaluation, the higher the confidence of gain in price. This vector can also be used to simulate an investment portfolio, as a portfolio can be seen as a stochastiv vector of assets.\n",
    "    * This requires an additional dimension input. Inputs must be stacked in a Tensor\n",
    "    * Activation: softmax\n",
    "    * Target: softmax(future prices / current prices)\n",
    "    \n",
    "* Application: Portfolio Recommendation\n",
    "    * A recommended portfolio distribution for cryptocurrency investment\n",
    "\n",
    "### 7. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies and Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "DAYS_BLK  = 60 # Number of input days for learning\n",
    "TARGETS   = [1, 10, 30] # Number of days to predict in the future\n",
    "TEST_DAYS = 15 # Number of most recent days to withold from the training set for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_samples(coins, DAYS_BLK, TARGETS, TEST_DAYS):\n",
    "    # Compile a list of training examples, accompanied by target values for future prices\n",
    "    train_samples = []\n",
    "    for coin in tqdm.tqdm(coins):\n",
    "        # Get all available price history for a given coin\n",
    "        coin_data = coin.read_history()\n",
    "\n",
    "        # Drop unneccessary columns\n",
    "        coin_data = coin_data.drop(['Volume', 'Market Cap', 'Unnamed: 0'], 1) \n",
    "\n",
    "        # Reverse date ordering so we predict the future instead of the past\n",
    "        coin_data['Date'] = pd.to_datetime(coin_data['Date'])\n",
    "        coin_data = coin_data.sort_values(['Date']) \n",
    "\n",
    "        # Convert Date to Day of Year(DOY)\n",
    "        coin_data['DOY'] = coin_data['Date'].apply(lambda x: x.dayofyear)\n",
    "        coin_data = coin_data.drop(['Date'], 1)\n",
    "\n",
    "        # Chunk coin history into input/target train pairs\n",
    "        for t in range(len(coin_data) - DAYS_BLK - TARGETS[-1] - TEST_DAYS):\n",
    "\n",
    "            # X is a 60 day chunk of shape: (60, 5)\n",
    "            X = coin_data.iloc[t:t+DAYS_BLK].copy()\n",
    "            # y is a list of future prices [1, 10, 30] days after X\n",
    "            y = [coin_data.iloc[t+DAYS_BLK+n]['Close'] for n in TARGETS]\n",
    "\n",
    "            # Normalize prices relative to the final input close price\n",
    "            price_cols = ['Open', 'High', 'Low', 'Close']\n",
    "            current_price = X.values[-1][3]\n",
    "            for col in price_cols:\n",
    "                X[col] = X[col] / current_price\n",
    "\n",
    "            # Normalize future prices by current price\n",
    "            y = [future_price / current_price for future_price in y]\n",
    "\n",
    "            # Normalize DOY column\n",
    "            X['DOY'] = X['DOY'] / 365\n",
    "\n",
    "            # Sample is ready\n",
    "            train_samples.append((X.values, y))\n",
    "    return train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coin data for the top 100 coins from coinmarketcap.com\n",
    "from Scrapers.Coinmarketcap.coinmarketcap import CoinMarketcap\n",
    "cmk = CoinMarketcap()\n",
    "coins = cmk.coins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Cache...\n",
      "Cached samples loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cache samples because compiling them is timely\n",
    "RECOMPILE_SAMPLES = False\n",
    "if RECOMPILE_SAMPLES:\n",
    "    samples = compile_samples(coins, DAYS_BLK, TARGETS, TEST_DAYS)\n",
    "    pickle.dump(samples, open( \"dataset.pickle\", \"wb\" ) )\n",
    "else:\n",
    "    try:\n",
    "        print(\"Checking Cache...\")\n",
    "        samples = pickle.load(open('dataset.pickle','rb'))\n",
    "        print(\"Cached samples loaded!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Cached samples not found. Fetching data and compiling samples...\")\n",
    "        samples = compile_samples(coins, DAYS_BLK, TARGETS, TEST_DAYS)\n",
    "        print(\"Caching samples...\")\n",
    "        pickle.dump(samples, open( \"dataset.pickle\", \"wb\" ) )\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, LSTM, Dropout\n",
    "\n",
    "# Assert input shape\n",
    "N = 60 # Number of Days\n",
    "F = 5  # Number of Features: ['Open', 'High', 'Low', 'Close', 'DayOfYear']\n",
    "assert((N, F) == samples[0][0].shape)\n",
    "\n",
    "binary_kwargs = {\n",
    "    'output':'sigmoid',\n",
    "    'metrics':['accuracy'],\n",
    "    'loss':'binary_crossentropy',\n",
    "}\n",
    "linear_kwargs = {\n",
    "    'output':'linear',\n",
    "    'metrics':['mae', 'mse'],\n",
    "    'loss':'mean_squared_error',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN(output='sigmoid', activations='selu', metrics=['accuracy'], loss='binary_crossentropy', optimizer='adam'):\n",
    "    # Define our Temporal Convolutional Model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=6, kernel_size=3, activation=activations, input_shape=[N,F]))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=6, kernel_size=3, activation=activations))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=28, activation=activations))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=activations))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation=activations))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation=activations))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=output)) # 'linear' for price prediction, 'softmax' for stochastic forecasting\n",
    "    \n",
    "    # Compile our Model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics) # Try other optimizers!\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM(output='sigmoid', activations='selu', metrics=['accuracy'], loss='binary_crossentropy', dropout=0.25, optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(8, input_shape=(N,F)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation=activations))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation=activations))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation=activations))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=output)) # 'linear' for price prediction, 'softmax' for stochastic forecasting\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binary Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many days into the future would you like to predict? Options: [1,10,30]\n",
    "n_days = 1\n",
    "target_index = TARGETS.index(n_days)\n",
    "\n",
    "# Split samples into inputs and targets\n",
    "X,y = zip(*samples)\n",
    "assert(len(X) == len(y))\n",
    "\n",
    "# target[0] is the price 1 day after input\n",
    "y = [target[0] for target in y]\n",
    "\n",
    "# convert target to boolean. 0 means price went down, 1 means price went up\n",
    "y = [int(target>1) for target in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/shared/CMPT/big-data/tmp_py/dlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CNN_model = build_CNN(**binary_kwargs, activations='elu')\n",
    "CNN_history = CNN_model.fit(np.array(X), np.array(y), epochs=5000, batch_size=4096, validation_split=0.1, verbose=0)\n",
    "print(\"Validation Accuracy: \", CNN_history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LSTM_model = build_LSTM(**binary_kwargs)\n",
    "LSTM_history = LSTM_model.fit(np.array(X), np.array(y), epochs=500, batch_size=4096, validation_split=0.1, verbose=0)\n",
    "print(\"Validation Accuracy: \", LSTM_history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.suptitle(\"Network Training Progress By Epochs\", fontsize=24)\n",
    "\n",
    "model_titles = ['Convolutional Network', 'LSTM Recurrent Network']\n",
    "metric_titles = {'acc':'Accuracy', 'loss':'Loss'}\n",
    "for col, metric in enumerate(['acc', 'loss']):\n",
    "    for row, history in enumerate([CNN_history, LSTM_history]):\n",
    "            ax = plt.subplot(2,2,(row)*2 + (col+1))\n",
    "            plt.plot(history.history[metric])\n",
    "            plt.plot(history.history['val_' + metric])\n",
    "            plt.title(model_titles[row] + \" \" + metric_titles[metric], fontsize=18)\n",
    "            plt.ylabel(metric_titles[metric])\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(['train', 'validation'], loc='upper left')\n",
    "# Fix subplot and title allignment\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models\n",
    "CNN_model.save('CNN_model_1.h5')\n",
    "LSTM_model.save('LSTM_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train Histories\n",
    "with open('CNN_history_1.pickle', 'wb') as file_pi:\n",
    "    pickle.dump(CNN_history.history, file_pi)\n",
    "with open('LSTM_history_1.pickle', 'wb') as file_pi:\n",
    "    pickle.dump(LSTM_history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Forecasting with Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cryptocurrency Investment Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
